{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7rz0JrCcLlAAPOJzTNRJU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 토큰화\n","- https://velog.io/@hyewon0309/LM-%ED%95%9C%EA%B5%AD%EC%96%B4-Tokenizer-Python"],"metadata":{"id":"1xwsYQVbLFIC"}},{"cell_type":"markdown","source":["### 어절 토큰화"],"metadata":{"id":"VerL6XidEpGW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"weOaAVjPEFcy"},"outputs":[],"source":["def word_tokenizer(s):\n","  s = re.compile(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\").sub('', s)\n","  \n","  result = s.split(' ')\n","  result = list(filter(None, result))   # 빈 리스트 요소('') 삭제\n","  return result"]},{"cell_type":"markdown","source":["### 음절 토큰화"],"metadata":{"id":"fmFv3wv-EtSX"}},{"cell_type":"code","source":["def syllable_tokenizer(s):\n","    s = re.compile(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\").sub('', s)\n","    \n","    result = []\n","    for c in s:\n","        result.append(c)\n","    return result"],"metadata":{"id":"3fYer_QFET9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 초성, 중성, 종성 토큰화"],"metadata":{"id":"HJOninf8ExOZ"}},{"cell_type":"code","source":["NO_CHOSUNG = '-'\n","NO_JOONGSUNG = '#'\n","NO_JONGSUNG = '*'\n","\n","CHOSUNGS = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ',\n","'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n","JOONGSUNGS = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ',\n","'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n","JONGSUNGS = [NO_JONGSUNG,  'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ',\n","'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n","\n","N_CHOSUNGS = 19\n","N_JOONGSUNGS = 21\n","N_JONGSUNGS = 28\n","\n","FIRST_HANGUL = 0xAC00 #'가'\n","LAST_HANGUL = 0xD7A3 #'힣'"],"metadata":{"id":"7EpuDomVEZBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def cho_joong_jong_tokenizer(s):\n","    result = []\n","\n","    for c in s:\n","      uni = ord(c) \n","      # 완전한 문자인 경우\n","      if uni >= FIRST_HANGUL and uni <= LAST_HANGUL:\n","          code = uni - FIRST_HANGUL\n","          jongsung_index = code % N_JONGSUNGS\n","          code //= N_JONGSUNGS\n","          joongsung_index = code % N_JOONGSUNGS\n","          code //= N_JOONGSUNGS\n","          chosung_index = code\n","\n","          result.append(CHOSUNGS[chosung_index])\n","          result.append(JOONGSUNGS[joongsung_index])\n","          result.append(JONGSUNGS[jongsung_index])\n","\n","      # 초성만 있는 경우\n","      elif c in CHOSUNGS :\n","        result = [c, NO_JOONGSUNG, NO_JONGSUNG]\n","      \n","      # 중성만 있는 경우\n","      elif c in JOONGSUNGS :\n","        result = [NO_CHOSUNG, c, NO_JONGSUNG]\n","      \n","      # 종성만 있는 경우\n","      elif c in JONGSUNGS :\n","        result = [NO_CHOSUNG, NO_JOONGSUNG, c]\n","\n","      else :\n","        return c\n","        \n","    return ''.join(result)"],"metadata":{"id":"V4iGMhYAEarM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 초중성, 종성 토큰화"],"metadata":{"id":"Een5sApEE3XV"}},{"cell_type":"code","source":["def chojoong_jong_tokenizer(s):\n","    result = []\n","    for c in s:\n","        if ord(c) < FIRST_HANGUL or ord(c) > LAST_HANGUL:\n","            # result.append(c)\n","            # 초성, 중성, 종성만 있는 것이나, 한글이 아닌 것은 무시됨\n","            pass\n","        else:\n","            code = ord(c) - FIRST_HANGUL\n","            jongsung_index = code % N_JONGSUNGS\n","\n","            chojoongsung_code = ord(c) - jongsung_index\n","            chojoongsung = chr(chojoongsung_code)\n","\n","            result.append(chojoongsung)\n","            result.append(JONGSUNGS[jongsung_index])\n","\n","    return ''.join(result)"],"metadata":{"id":"YW0EoIk5EcIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### word_cjs_tokenizer : 어절로 나눈 단어를 초중종 토큰화"],"metadata":{"id":"4oMiB5ISre0N"}},{"cell_type":"code","source":["import re"],"metadata":{"id":"DT6zWzK4xWZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_cjs_tokenizer(s) :\n","  # 정규표현식\n","  s = re.compile(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\").sub('', s)\n","  \n","  words = s.strip().split(' ')\n","\n","  result = []\n","  for word in words :\n","    tokenized_word = cho_joong_jong_tokenizer(word)\n","    result.append(tokenized_word)\n","\n","  result = list(filter(None, result))\n","  return result"],"metadata":{"id":"nl0XSFnpt2xB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### syllable_cjs_tokenizer : 음절로 나눈 단어를 초중종 토큰화"],"metadata":{"id":"47vUNTNdtxxn"}},{"cell_type":"code","source":["def syllable_cjs_tokenizer(s) :\n","  # 정규표현식\n","  s = re.compile(\"[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]\").sub('', s)\n","  \n","  words = syllable_tokenizer(s)\n","\n","  result = []\n","  for word in words :\n","    tokenized_word = cho_joong_jong_tokenizer(word)\n","    result.append(tokenized_word)\n","\n","  result = list(filter(None, result))\n","  return result"],"metadata":{"id":"XOoT7sfBrhR_"},"execution_count":null,"outputs":[]}]}
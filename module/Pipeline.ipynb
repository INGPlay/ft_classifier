{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYEeneJR+LZEAX12OnIa/f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Pipeline\n","\n","```\n","custom_tokenizer = Custom_Tokenizer3의 함수 객체\n","\n","tokenized_texts = pl.tokenization(texts, custom_tokenizer)\n","int_texts = pl.texts_to_int(tokenized_texts, tokenizer_tf)\n","```"],"metadata":{"id":"XoHVWo1OydPZ"}},{"cell_type":"markdown","source":["### tokenization : 토큰화"],"metadata":{"id":"TmqCYvVgwHlf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLQPNU4bwBOR"},"outputs":[],"source":["# 토큰화 함수\n","## callback : tokenizer 함수 중 하나\n","def tokenization(X, callback) :\n","  tokenized_X = []\n","  for sentence in X.copy() :\n","    tokenized_part = callback(sentence)\n","    tokenized_X.append(tokenized_part)\n","\n","  return tokenized_X"]},{"cell_type":"markdown","source":["### texts_to_int : 문자표현 → 정수표현"],"metadata":{"id":"P-aZimnfwLza"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np"],"metadata":{"id":"TI9DKV8TwKQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def texts_to_int(tokenized_X, tokenizer_tf, max_len = 50) :\n","  int_X = tokenizer_tf.texts_to_sequences(tokenized_X)\n","\n","  X_np = np.array(int_X)\n","\n","  pad_X = pad_sequences(X_np, maxlen = max_len, padding='post')\n","\n","  return pad_X"],"metadata":{"id":"ct8euYF2wLPa"},"execution_count":null,"outputs":[]}]}